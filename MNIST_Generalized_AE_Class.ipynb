{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Generalized AE Class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWAFG5cAxU/7PevFeYvE3X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3l8K2xV-xlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7af62d07-e446-46f9-a29f-bc4048191fd7"
      },
      "source": [
        "# Getting classy\n",
        "# import the necessary packages\n",
        "# from tensorflow.keras.layers import LeakyReLU\n",
        "# from tensorflow.keras.layers import Activation\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Reshape, Input, Dropout\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "# from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "class flatAutoencoder:\n",
        "  @staticmethod\n",
        "  def build(input_shape, layer_dims=(512, 256, 128, 32), \n",
        "            enc_dropout_reg = None, dec_dropout_reg = None,\n",
        "            enc_l2_reg = None, dec_l2_reg = None,\n",
        "            enc_l1_reg = None, dec_l1_reg = None):\n",
        "    '''\n",
        "    enc_dropout_reg: takes tuple size of layer_dims and with the desired dropout\n",
        "    in the correct index following encoding layer. e.g. (0.5,None,None) if a \n",
        "    dropout of 0.5 is desired after the initial layer in given default layer_dims.\n",
        "    dec_dropout_reg: takes tuple size of layer_dims and desired dropout in \n",
        "    index following decoder layers. e.g.(0.5,None,None) in given default\n",
        "    layer_dims. Currently can't do the last layer but that's small as most models\n",
        "    performed worse with that layer.\n",
        "\n",
        "    enc_l2_reg: l2 regularization added to encoder layers if specified. Takes a \n",
        "    tuple e.g. (10e-5, None, None) for regularization of the corresponding layer.\n",
        "    dec_l2_reg: l2 regularization added to decoder layers if specified. Takes a \n",
        "    tuple e.g. (10e-5, None, None) for regularization of the corresponding layer.\n",
        "\n",
        "    enc_l1_reg: l1 regularization added to encoder layers if specified. Takes a \n",
        "    tuple e.g. (10e-5, None, None) for regularization of the corresponding layer.\n",
        "    dec_l1_reg: l1 regularization added to decoder layers if specified. Takes a \n",
        "    tuple e.g. (10e-5, None, None) for regularization of the corresponding layer.\n",
        "    '''\n",
        "    # define the input to the encoder\n",
        "    inputs = Input(shape=(input_shape,))\n",
        "    x = inputs\n",
        "\n",
        "\t\t# loop over the number of layer_dims to add sets of layers\n",
        "    for i, dim in enumerate(layer_dims):\n",
        "      # Check l2 regularization\n",
        "      if enc_l2_reg is not None:\n",
        "        if enc_l2_reg[i]:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                    activity_regularizer=regularizers.l2(enc_l2_reg[i]))(x)\n",
        "        else:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "      \n",
        "      # Check l1 regularization\n",
        "      elif enc_l1_reg is not None:\n",
        "        if enc_l1_reg[i]:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                    activity_regularizer=regularizers.l1(enc_l1_reg[i]))(x)\n",
        "        else:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "\n",
        "      # Non-regularized layer if no l2/l1\n",
        "      else:\n",
        "        x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "\n",
        "      # Introduce dropout if specified\n",
        "      if enc_dropout_reg is not None:\n",
        "        if enc_dropout_reg[i]:\n",
        "          x = Dropout(enc_dropout_reg[i])(x)\n",
        "    \n",
        "    # build encoder model\n",
        "    encoder = Model(inputs, x, name=\"encoder\")\n",
        "    \n",
        "    # Building decoder model\n",
        "    latentInputs = Input(shape=(layer_dims[-1],))\n",
        "    x = latentInputs\n",
        "\n",
        "    # loop over our number of layer_dims again, but in reverse order\n",
        "    for i, dim in reversed(list(enumerate(layer_dims[:-1]))):\n",
        "      # Introduce dropout if specified\n",
        "      if dec_dropout_reg is not None:\n",
        "        if dec_dropout_reg[i]:\n",
        "          x = Dropout(dec_dropout_reg[i])(x)\n",
        "\n",
        "      # Check l2 regularization\n",
        "      if dec_l2_reg is not None:\n",
        "        if dec_l2_reg[i]:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                    activity_regularizer=regularizers.l2(dec_l2_reg[i]))(x)\n",
        "        else:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "      \n",
        "      # Check l1 regularization\n",
        "      elif dec_l1_reg is not None:\n",
        "        if dec_l1_reg[i]:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                    activity_regularizer=regularizers.l1(dec_l1_reg[i]))(x)\n",
        "        else:\n",
        "          x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "\n",
        "      # Non-regularized layer if no l2/l1\n",
        "      else:\n",
        "        x = Dense(dim, activation='relu', \n",
        "                      activity_regularizer=None)(x)\n",
        "      # x = LeakyReLU(alpha=0.2)(x) -> might want to try this out later\n",
        "      \n",
        "    # apply linear activation (in this case) for output\n",
        "    outputs = Dense(input_shape, activation='linear')(x)\n",
        "    \n",
        "    # build the decoder model\n",
        "    decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
        "\n",
        "    # autoencoder is the encoder wrapped by decoder\n",
        "    autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
        "                        name=\"autoencoder\")\n",
        "\n",
        "    # return a 3-tuple of the encoder, decoder, and autoencoder\n",
        "    return (encoder, decoder, autoencoder)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivSG542Q-DkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cb59e053-d489-4d86-f401-cd1232afdd08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoB0j62_mXhG",
        "colab_type": "text"
      },
      "source": [
        "TODO:\n",
        "1. ~Create method to integrate regularization into encoding or decoding layer~\n",
        "2. ~Fix error that makes layers smaller for some reason (see below layer summaries)~\n",
        "3. ~Change layer_dims so that it includes the encoding dimension to simplify the code. Ridiculous that I would need to make a special argument just for the encoding dimension - it's the smallest layer. Just use that info.~\n",
        "4. ~Verify that regularization is built into the layer~\n",
        "5. Automatic file saving during training.\n",
        "\n",
        "Future work:\n",
        "1. Make tests for if more dropouts are provided than can be used\n",
        "2. Create error if regularization are put in the same spot in the network - e.g. whenever the tuples for both l1 and l2 encoder regularzation are the same\n",
        "3. Allow different outputs from the model, such as softmax\n",
        "4. Implement elastic net regularization\n",
        "5. Create optional argument to share parameters betwee encoding/decoding layers\n",
        "6. Fix dropout tuple length error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSlOSYi7e8ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "4775326e-18b5-4e7b-f488-a3d7a792c913"
      },
      "source": [
        "layer_dims=(512, 256, 128, 32)\n",
        "enc_dropout_reg = (0.5,0.4,0.3,0.2)\n",
        "dec_dropout_reg = (0.5,0.5,0.5,0.5)\n",
        "enc_l2_reg = (1e-1,1e-1,1e-1,1e-1)\n",
        "dec_l2_reg = (1e-2,1e-2,1e-2,1e-2)\n",
        "# enc_l1_reg = (1e-3,1e-3,1e-3,1e-3)\n",
        "enc_l1_reg = (None,None,None,None)\n",
        "# dec_l1_reg = (1e-4,1e-4,1e-4,1e-4)\n",
        "dec_l1_reg = (None,None,None,None)\n",
        "\n",
        "# Think I'm going to need to zip forward/backward direction items together\n",
        "# forward_pass = list(zip(layer_dims[:-1], enc_l1_reg, enc_l2_reg, enc_dropout_reg))\n",
        "# forward_pass\n",
        "# backward_pass = list(zip(layer_dims[::-1], dec_l1_reg, dec_l2_reg, dec_dropout_reg))\n",
        "# backward_pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(32, None, 0.01, 0.5),\n",
              " (128, None, 0.01, 0.5),\n",
              " (256, None, 0.01, 0.5),\n",
              " (512, None, 0.01, 0.5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJCTPEIigswa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a6c0d4d6-68f6-4b84-fcd5-5ae7fd9d192f"
      },
      "source": [
        "forward_pass = list(zip(layer_dims[:-1], enc_l1_reg, enc_l2_reg, enc_dropout_reg))\n",
        "backward_pass = list(zip(layer_dims[::-1], dec_l1_reg, dec_l2_reg, dec_dropout_reg))\n",
        "\n",
        "dummy = []\n",
        "for forward_layer in forward_pass:\n",
        "  dummy.append('-'.join(str(v) for v in forward_layer))\n",
        "forward_name = '_'.join(str(v) for v in dummy)\n",
        "\n",
        "dummy2 = []\n",
        "for backward_layer in backward_pass:\n",
        "  dummy2.append('-'.join(str(v) for v in backward_layer))\n",
        "backward_name = '_'.join(str(v) for v in dummy2)\n",
        "\n",
        "model_name = '_'.join((forward_name, backward_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'512-None-0.1-0.5_256-None-0.1-0.4_128-None-0.1-0.3_32-None-0.01-0.5_128-None-0.01-0.5_256-None-0.01-0.5_512-None-0.01-0.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oROFksFgxVXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3337ca63-2ee7-4107-d8b3-33acd50b1de5"
      },
      "source": [
        "layer_file"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'512-256-128-32'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1FpYZtnOnMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "776a3c3e-da70-4788-f8d3-8815577bba0e"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# from keras.layers import Input, Dense\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dropout\n",
        "# from keras import regularizers\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
        "import h5py\n",
        "import os\n",
        "import pathlib\n",
        "from datetime import datetime\n",
        "\n",
        "date = datetime.today().strftime('%Y-%m-%d')\n",
        "study_name = \"MNIST_Example\"\n",
        "optimizer_metric = \"RMS_prop-MSE\"\n",
        "\n",
        "layer_dims=(512, 256, 128, 32)\n",
        "enc_dropout_reg = (None,None,None,None)\n",
        "dec_dropout_reg = (None,None,None,None)\n",
        "enc_l2_reg = (None,None,None,None)\n",
        "dec_l2_reg = (None,None,None,None)\n",
        "enc_l1_reg = (None,None,None,None)\n",
        "dec_l1_reg = (None,None,None,None)\n",
        "\n",
        "# Filename specifying layers\n",
        "layer_file = '-'.join(str(v) for v in layer_dims)\n",
        "\n",
        "# Logic for creating model name\n",
        "forward_pass = list(zip(layer_dims[:-1], enc_l1_reg, enc_l2_reg, enc_dropout_reg))\n",
        "backward_pass = list(zip(layer_dims[::-1], dec_l1_reg, dec_l2_reg, dec_dropout_reg))\n",
        "\n",
        "dummy = []\n",
        "for forward_layer in forward_pass:\n",
        "  dummy.append('-'.join(str(v) for v in forward_layer))\n",
        "forward_name = '_'.join(str(v) for v in dummy)\n",
        "\n",
        "dummy2 = []\n",
        "for backward_layer in backward_pass:\n",
        "  dummy2.append('-'.join(str(v) for v in backward_layer))\n",
        "backward_name = '_'.join(str(v) for v in dummy2)\n",
        "\n",
        "model_name = '_'.join((forward_name, backward_name))\n",
        "\n",
        "# Building the autoencoder\n",
        "(encoder, decoder, autoencoder) = flatAutoencoder.build(input_shape=784, \n",
        "                                      layer_dims=layer_dims,\n",
        "                                      enc_dropout_reg=enc_dropout_reg, \n",
        "                                      dec_dropout_reg=dec_dropout_reg,\n",
        "                                      enc_l1_reg=enc_l1_reg,\n",
        "                                      enc_l2_reg=enc_l2_reg,\n",
        "                                      dec_l1_reg=dec_l1_reg,\n",
        "                                      dec_l2_reg=dec_l2_reg)\n",
        "\n",
        "# Configuring optimizer and loss function\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, momentum=0)\n",
        "autoencoder.compile(optimizer=opt, \n",
        "                    loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "# Path library\n",
        "file_path = \"./drive/My Drive/Lab/BMP AE Models/Models/{} AE/{}/{}/{}\".format(layer_file, \n",
        "                                                                               study_name, \n",
        "                                                                               date, \n",
        "                                                                               optimizer_metric)\n",
        "pathlib.Path(file_path).mkdir(parents=True, exist_ok=True)\n",
        "drive_file = file_path + '/' + model_name + '_model.h5'\n",
        "\n",
        "# Early stopping, checkpointing, and csv logger\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
        "mc = ModelCheckpoint(filepath=drive_file, \n",
        "                     monitor='val_loss', mode='min', \n",
        "                     verbose=0, save_best_only=False) # Don't only save best\n",
        "csv_logger = CSVLogger(drive_file[:-3]+'history_log.csv', append=True)\n",
        "\n",
        "# Check on models\n",
        "# print(encoder.summary())\n",
        "# print(decoder.summary())\n",
        "# print(autoencoder.summary())\n",
        "\n",
        "# Loading MNIST data\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Train\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                          epochs=10,\n",
        "                          batch_size=512,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_test, x_test),\n",
        "                          callbacks=[es, mc, csv_logger])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 39us/step - loss: 0.0614 - val_loss: 0.0453\n",
            "Epoch 2/10\n",
            "15872/60000 [======>.......................] - ETA: 0s - loss: 0.0454"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "  'TensorFlow optimizers do not '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.0416 - val_loss: 0.0369\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 12us/step - loss: 0.0348 - val_loss: 0.0350\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 12us/step - loss: 0.0313 - val_loss: 0.0299\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.0289 - val_loss: 0.0266\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.0271 - val_loss: 0.0242\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.0255 - val_loss: 0.0247\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkZBjaJbflTr",
        "colab_type": "text"
      },
      "source": [
        "I don't think the callbacks are working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxnEcf1YejqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "75ea9470-684c-41d2-a4ac-83fe8504d981"
      },
      "source": [
        "file_path = \"./drive/My Drive/{} AE/{}/{}/{}\".format(layer_file, \n",
        "                                                                               study_name, \n",
        "                                                                               date, \n",
        "                                                                               optimizer_metric)\n",
        "pathlib.Path(file_path).mkdir(parents=True, exist_ok=True)\n",
        "drive_file = file_path + '/' + model_name + '_model.h5'\n",
        "print(file_path)\n",
        "print(drive_file)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-215d50ebd14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                                \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                                optimizer_metric)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdrive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'drive/My Drive/512-256-128-32 AE/MNIST_Example/2020-07-20/RMS_prop-MSE'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6mv1rrHejix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVL3rxoejZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW3rBWP_ZGB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c3adfb65-a0ef-41fb-f413-aa20ce547099"
      },
      "source": [
        "for layer in encoder.layers:\n",
        "  # print(layer)\n",
        "  print(layer.get_config())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_input_shape': (None, 748), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_5'}\n",
            "{'name': 'dense_16', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_17', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 128, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_19', 'trainable': True, 'dtype': 'float32', 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niY2urbr1_8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "ace4df22-c797-4e26-e302-1d1a13660530"
      },
      "source": [
        "for layer in decoder.layers:\n",
        "  # print(layer)\n",
        "  print(layer.get_config())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_input_shape': (None, 32), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_6'}\n",
            "{'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'noise_shape': None, 'seed': None}\n",
            "{'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 128, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'class_name': 'L1L2', 'config': {'l1': 9.999999747378752e-05, 'l2': 0.0}}, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}\n",
            "{'name': 'dense_19', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'class_name': 'L1L2', 'config': {'l1': 4.0, 'l2': 0.0}}, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dropout_9', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}\n",
            "{'name': 'dense_20', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'class_name': 'L1L2', 'config': {'l1': 5.0, 'l2': 0.0}}, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 1728, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5BELDAkbfyW",
        "colab_type": "text"
      },
      "source": [
        "I really should look into making a CNN for this. So many less parameters with the CNN.\n",
        "\n",
        "I now need to:\n",
        "1. figure out how to choose where and what type of regularization occurs for each layer. The logic of the for loop is going to get a little more complicated by having "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_DYcpOkPsNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2c156186-2d23-4df6-e6ec-ca54f390f0de"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvRDI34NPsLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "08ed6ea6-b76b-45f7-89a5-e9c592c6c4db"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# this is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# this is our input placeholder\n",
        "input_img = Input(shape=(784,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = Dense(encoding_dim, activation='relu',\n",
        "                activity_regularizer=None)(input_img)\n",
        "encoded = Dropout(0.4)(encoded)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "# autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "\n",
        "# autoencoder.fit(x_train, x_train,\n",
        "#                 epochs=50,\n",
        "#                 batch_size=256,\n",
        "#                 shuffle=True,\n",
        "#                 validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 32)                25120     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 784)               25872     \n",
            "=================================================================\n",
            "Total params: 50,992\n",
            "Trainable params: 50,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEyYjdz9PsJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CvNHfwzPsHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHmocEX9PsEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz5xc92nPsAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650z3vfxPr-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4CqJMLwPr9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJgFlEIfPr6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgRSdr2uPr4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBfoSmOPr2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZXzgz_oPrzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}